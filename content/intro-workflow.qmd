---
title: Workflow
---

```{r setup, include=FALSE}
#| file: functions.R
```

## Data workflow from boat to production

Below is a summary of the workflow:

```{mermaid}
%%| label: fig-workflow
%%| fig-cap: "Simplified data workflow from boat to production."

flowchart LR
  A[Catch data\ncollection tablet] --> B[METIS]
  A1[Length data\ncollection tablets] --> B
  A2[Specimen data\ncollection tablet] --> B
  A3[Wheelhouse\nhaul data] --> C[GIDES and Oracle]
  B --> C
  C --> D[RACEBASE and RACE_DATA\nOracle schema]
  D --> E[gapindex\nR package]
  D --> F[gap_products\nR scripts]
  E --> F
  F --> G[Production tables\nin GAP_PRODUCTS\nOracle schema]
  E --> G
  G --> H[Data process\nreports &\npresentations]
  G --> I[AKFN]
  G --> J[FOSS]
  
```

**Once survey data collected on the vessel has been checked and validated**, the `code/run.R` script (within the `gap_products` repository) houses the sequence of programs that calculate the standard data products resulting from the NOAA AFSC GAP bottom trawl surveys. Standard data products are the CPUE, BIOMASS, SIZECOMP, and AGECOMP tables in the GAP_PRODUCTS Oracle schema. The tables are slated to be updated twice a year, once after the survey season following finalization of that summer's bottom trawl survey data to incorporate the new catch, size, and effort data and once prior to an upcoming survey to incorporate new age data that were processed after the prior summer's survey season ended. This second pre-survey production run will also incorporate changes in the data due to the specimen voucher process as well as other post-hoc changes in the survey data.

1.  Import versions of the tables in GAP_PRODUCTS locally within the gap_products repository to compare with the updated production tables. Any changes to a production table will be compared and checked to make sure those changes are intentional and documented.

2.  Use the `gapindex` R package to calculate the four major standard data products: CPUE, BIOMASS, SIZECOMP, AGECOMP. These tables are compared and checked to their respective locally saved copies and any changes to the tables are vetted and documented. These tables are then uploaded to the GAP_PRODUCTS Oracle schema.

3.  Calculate the various materialized views for AKFIN and FOSS purposes. Since these are derivative of the tables in GAP_PRODUCTS as well as other base tables in RACEBASE and RACE_DATA, it is not necessary to check these views in addition to the data checks done in the previous steps.

## Operational Product Development Timeline

```{r prod-timeline}
#| tbl-cap: Operational product development timeline. 

dat <- data.frame(Month = unique(months(as.Date(x = 1:365, format = "%m"))), 
           Surveys = 
             c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0), 
           Planning = 
             c(1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1), 
           Development = 
             c(1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1), 
           deployment_deliverables = # Deployment (survey deliverables)
             c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1), 
           deployment_operations = # Deployment (survey operations)
             c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0), 
           triage = 1, # Triage (fixing bugs and errors)
           feedback = 1 # User feedback and brainstorming
           ) %>% 
  t() %>% 
  data.frame()

names(dat) <- dat[1,]
dat <- dat[-1,]
cols  <- viridis_pal(option = "G", begin = .2, end = .8)(nrow(dat))
# dat$cols <- cols
dat[dat == 0] <- ""
# for (i in 1:nrow(dat)) {
#   dat[i,][dat == TRUE] <- rownames(dat)[i]
# }
dat <- cbind.data.frame(" " = c("Surveys", "Planning", "Development", "Deployment (survey deliverables)", "Deployment (survey operations)", "Triage (fixing bugs and errors)", "User feedback and brainstorming"), 
                        dat)
t0 <- dat %>% 
  flextable::flextable() %>% # col_keys = names(dat)[-ncol(dat)]
  flextable::vline(j = 1)

for (i in 1:nrow(dat)){
  for (j in 2:(ncol(dat))) {
    if (dat[i,j] == 1) {
t0 <- t0 %>% 
  flextable::bg(i = i, j = j, bg = cols[i], part = "body")  %>% 
  flextable::color(i = i, j = j, color = cols[i], part = "body") 
    }
  }
}

t0
```

## Where are these data used?

```{mermaid}
%%| label: fig-data-used
%%| fig-cap: "Major users of the GAP data product tables. "

flowchart LR
  A[gap_products data tables] --> B[Reports]
  B --> C[Data process\nreports &\npresentations]
  B --> G[Ecosystem Status Reports]
  A --> D[Outreach]
  D --> E[Plan Team Presentations]
  D --> F[Community\nHighlight\nDocuments]
  D --> H[Survey progress and temperature maps]
  A --> I[Data Platforms]
  I --> J[AKFN]
  I --> K[FOSS]
  I --> L[afscgap Python package]
  A --> M[VAST Model-Based Indices]

```

## Data levels

GAP produces numerous data products that are subjected to different levels of processing, ranging from raw to highly-derived. The suitability of these data products for analysis varies and there is ambiguity about which data products can be used for which purpose. This ambiguity can create challenges in communicating about data products and potentially lead to misunderstanding and misuse of data. One approach to communicating about the level of processing applied to data products and their suitability for analysis is to describe data products using a Data Processing Level system. Data Processing Level systems are widely used in earth system sciences to characterize the extent of processing that has been applied to data products. For example, the NOAA National Centers for Environmental Information (NCEI) Satellite Program uses a Data Processing Level system to describe data on a scale of 0-4, where Level 0 is raw data and Level 4 is model output or results from analysis. Example of how [NASA remote sensing data products](https://ladsweb.modaps.eosdis.nasa.gov/search/) are shared through a public data portal with levels of data processing and documentation.

For more information, see [Sean Rohan's October 2022 SCRUGS presentation](https://docs.google.com/presentation/d/1rWSZpeghWJqzWMIa5oBc4BCoy-zy1Yue86RoTw58u6M/edit?usp=sharing) on the topic.

-   **Level 0**: Raw and unprocessed data. Ex: Data on the G drive, some tables in RACE_DATA
-   **Level 1A**: Data products with QA/QC applied that may or may not be expanded to analysis units, but either not georeferenced or does not include full metadata. Ex: Some tables in RACE_DATA and RACEBASE
-   **Level 2**: Analysis-ready data products that are derived for a standardized extent and account for zeros and missing/bad data. Ex: CPUE tables, some data products in public-facing archives and repositories
-   **Level 3**: Data products that are synthesized across a standardized extent, often inputs in a higher-level analytical product. Ex: Abundance indices, some data products in public-facing archives and repositories
-   **Level 4**: Analytically generated data products that are derived from lower-level data, often to inform management. Ex: Biological reference points from stock assessments, Essential Fish Habitat layers, indicators in Ecosystem Status Reports and Ecosystem and Socioeconomic Profiles
